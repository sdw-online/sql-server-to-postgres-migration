{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70c3b01c",
   "metadata": {},
   "source": [
    "# Data Migration: SQL Server to Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97934904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ddb985",
   "metadata": {},
   "source": [
    "## 1. Load credentials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d66e6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02903d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_host = os.getenv(\"SQL_SERVER_HOST\")\n",
    "sql_db = os.getenv(\"SQL_SERVER_DB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26d2c553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL SERVER HOST: LAPTOP-1O17LCQR\\SQLEXPRESS\n",
      "SQL SERVER DB: TransactionDB_UAT\n"
     ]
    }
   ],
   "source": [
    "print(f\"SQL SERVER HOST: {sql_host}\")\n",
    "print(f\"SQL SERVER DB: {sql_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cc0d13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_host = os.getenv(\"POSTGRES_HOST\")\n",
    "pg_port = os.getenv(\"POSTGRES_PORT\")\n",
    "pg_db = os.getenv(\"POSTGRES_DB\")\n",
    "pg_user = os.getenv(\"POSTGRES_USER\")\n",
    "pg_password = os.getenv(\"POSTGRES_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f45b0c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSGRES HOST: localhost\n",
      "POSGRES PORT: 5432\n",
      "POSGRES DB: transaction_uat\n",
      "POSGRES USER: postgres\n",
      "POSGRES PASSWORD: password\n"
     ]
    }
   ],
   "source": [
    "print(f\"POSGRES HOST: {pg_host}\")\n",
    "print(f\"POSGRES PORT: {pg_port}\")\n",
    "print(f\"POSGRES DB: {pg_db}\")\n",
    "print(f\"POSGRES USER: {pg_user}\")\n",
    "print(f\"POSGRES PASSWORD: {pg_password}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118b931f",
   "metadata": {},
   "source": [
    "## 2. Connect to SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84546864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to SQL Server...\n",
      "   Server: LAPTOP-1O17LCQR\\SQLEXPRESS\n",
      "   Database: TransactionDB_UAT\n"
     ]
    }
   ],
   "source": [
    "print(\"Connecting to SQL Server...\")\n",
    "print(f\"   Server: {sql_host}\")\n",
    "print(f\"   Database: {sql_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c67f477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] -> Conenction to SQL Server now live! \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sql_conn_string = (\n",
    "        f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "        f\"SERVER={sql_host};\"\n",
    "        f\"DATABASE={sql_db};\"\n",
    "        f\"Trusted_Connection=yes;\"\n",
    "    )\n",
    "\n",
    "    sql_conn = pyodbc.connect(sql_conn_string)\n",
    "    sql_cursor = sql_conn.cursor()\n",
    "    print(\"[SUCCESS] -> Conenction to SQL Server now live! \")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"SQL Server connection failed: {e}\")\n",
    "    print(\"\"\" How to troubleshoot:\n",
    "          > 1. Check server name in .env file is correct\n",
    "          > 2. Verify SQL Server is running\n",
    "          > 3. Check Windows Authentication is enabled \n",
    "            .... \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86def7a0",
   "metadata": {},
   "source": [
    "## 3. Connect to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48a51a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to PostgreSQL...\n",
      "   Server: localhost\n",
      "   Database: transaction_uat\n"
     ]
    }
   ],
   "source": [
    "print(\"Connecting to PostgreSQL...\")\n",
    "print(f\"   Server: {pg_host}\")\n",
    "print(f\"   Database: {pg_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8472fafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to PostgreSQL\n",
      "   Version: PostgreSQL 18.0 on x86_64-windows, compiled by msv...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pg_conn = psycopg2.connect(\n",
    "        host=pg_host,\n",
    "        port=pg_port,\n",
    "        database=pg_db,\n",
    "        user=pg_user,\n",
    "        password=pg_password\n",
    "    )\n",
    "\n",
    "\n",
    "    pg_cursor=pg_conn.cursor()\n",
    "    pg_cursor.execute(\"SELECT version();\")\n",
    "\n",
    "    pg_version = pg_cursor.fetchone()[0]\n",
    "\n",
    "    print(\"Connected to PostgreSQL\")\n",
    "    print(f\"   Version: {pg_version[:50]}...\\n\")\n",
    "\n",
    "\n",
    "except psycopg2.OperationalError as e:\n",
    "    print(f\" Postgres connection failed: {e}\")\n",
    "    print(\"\"\" How to troubleshoot:\n",
    "            > 1. Check Postgres is running\n",
    "            > 2. Verify username + password\n",
    "            > 3. Check database exists\n",
    "          \n",
    "          ... \n",
    "\n",
    "\"\"\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n",
    "    raise "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af59af3",
   "metadata": {},
   "source": [
    "## 4. Define the tables to migrate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380c3191",
   "metadata": {},
   "source": [
    "### Migration order \n",
    "\n",
    "- Categories (no dependencies )\n",
    "- Suppliers (no dependencies)\n",
    "- Customers (no dependencies)\n",
    "- Products (depends on Categories and Suppliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c1ace80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Categories', 'Suppliers', 'Customers', 'Products']\n"
     ]
    }
   ],
   "source": [
    "tables_to_migrate = ['Categories', 'Suppliers', 'Customers', 'Products']\n",
    "print(tables_to_migrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "147082a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table to migrate:\n",
      "   1. Categories\n",
      "   2. Suppliers\n",
      "   3. Customers\n",
      "   4. Products\n",
      "\n",
      "Total no of tables to migrate: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Table to migrate:\")\n",
    "for i, table in enumerate(tables_to_migrate, 1):\n",
    "    print(f\"   {i}. {table}\")\n",
    "\n",
    "total_no_tbls = len(tables_to_migrate)\n",
    "print(f\"\\nTotal no of tables to migrate: {total_no_tbls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdbbbb2",
   "metadata": {},
   "source": [
    "## 5. Run pre-migration checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d0b84aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      ">>> Check 1: ROW COUNTS\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\">>> Check 1: ROW COUNTS\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9504d55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories                 8 rows\n",
      "Suppliers               5000 rows\n",
      "Customers             900000 rows\n",
      "Products              150000 rows\n",
      "------------------------------\n",
      "TOTAL              1,055,008 rows \n",
      "\n",
      " Baseline captured! \n"
     ]
    }
   ],
   "source": [
    "baseline_counts = {}\n",
    "\n",
    "\n",
    "try:\n",
    "    for table in tables_to_migrate:\n",
    "        quoted_table = f\"[{table}]\"\n",
    "        row_count_query = f\"SELECT COUNT(*) as total_rows FROM {quoted_table}\" \n",
    "        sql_cursor.execute(row_count_query)\n",
    "        count = sql_cursor.fetchone()[0]\n",
    "\n",
    "        baseline_counts[table] = count\n",
    "        print(f\"{table:15} {count:>12} rows\")\n",
    "\n",
    "    total_rows = sum(baseline_counts.values())\n",
    "    print(f\"{'-' * 30}\")\n",
    "    print(f\"{'TOTAL':15} {total_rows:>12,} rows \")\n",
    "    print(\"\\n Baseline captured! \")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to get baseline counts: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e8b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "55c48f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CHECK 2: NULL CHECKS (CustomerName)\n",
      "\n",
      "CHECK 3: INVALID EMAIL FORMATS CHECK\n",
      "\n",
      "CHECK 4: NEGATIVE PRODUCT PRICES CHECK\n",
      "\n",
      "CHECK 5: NEGATIVE STOCK QUANTITIES CHECK\n",
      "\n",
      "CHECK 6: ORPHANED FOREIGN KEYS CHECK\n",
      "\n",
      "CHECK 7: FUTURES DATES CHECK\n",
      "\n",
      "Data quality issues found (will migrate as-is)\n",
      "    > 4,514 customers with NULL names...\n",
      "    > 8,844 emails with invalid email formats...\n",
      "    > 775 prices contain negative prices...\n",
      "    > 1,467 products with negative stock...\n",
      "    > 24,700 products with orphaned foreign keys...\n",
      "    > 9,076 customers with future creations data later than current date...\n"
     ]
    }
   ],
   "source": [
    "quality_issues = []\n",
    "\n",
    "try:\n",
    "    print(\"\\nCHECK 2: NULL CHECKS (CustomerName)\")\n",
    "    sql_cursor.execute(\"\"\"SELECT COUNT(*) AS null_count \n",
    "                          FROM Customers \n",
    "                          WHERE CustomerName IS NULL\"\"\")\n",
    "    null_names = sql_cursor.fetchone()[0]\n",
    "    if null_names > 0:\n",
    "        quality_issues.append(f\"    > {null_names:,} customers with NULL names...\")\n",
    "    # print(quality_issues)\n",
    "\n",
    "    \n",
    "    print(\"\\nCHECK 3: INVALID EMAIL FORMATS CHECK\")\n",
    "    sql_cursor.execute(\"\"\"SELECT COUNT(*) AS invalid_email_count \n",
    "                            FROM Customers \n",
    "                            WHERE Email LIKE '%@invalid'  \"\"\")\n",
    "    invalid_emails = sql_cursor.fetchone()[0]\n",
    "    if invalid_emails > 0:\n",
    "        quality_issues.append(f\"    > {invalid_emails:,} emails with invalid email formats...\")\n",
    "    # print(quality_issues)\n",
    "    \n",
    "    print(\"\\nCHECK 4: NEGATIVE PRODUCT PRICES CHECK\")\n",
    "    sql_cursor.execute(\"\"\" SELECT COUNT(*) AS negative_product_prices_count \n",
    "                            FROM Products \n",
    "                            WHERE UnitPrice < 0\n",
    "                       \"\"\")\n",
    "    negative_price = sql_cursor.fetchone()[0]\n",
    "    if negative_price > 0:\n",
    "        quality_issues.append(f\"    > {negative_price:,} prices contain negative prices...\")\n",
    "    # print(quality_issues)\n",
    "\n",
    "\n",
    "    print(\"\\nCHECK 5: NEGATIVE STOCK QUANTITIES CHECK\")\n",
    "    sql_cursor.execute(\"\"\" SELECT COUNT(*) AS negative_stock_quanities_count \n",
    "                            FROM Products \n",
    "                            WHERE StockQuantity < 0\n",
    "                       \"\"\")\n",
    "    negative_stock_quantities = sql_cursor.fetchone()[0]\n",
    "    if negative_stock_quantities > 0:\n",
    "        quality_issues.append(f\"    > {negative_stock_quantities:,} products with negative stock...\")\n",
    "    # print(quality_issues)\n",
    "\n",
    "    print(\"\\nCHECK 6: ORPHANED FOREIGN KEYS CHECK\")\n",
    "    sql_cursor.execute(\"\"\" SELECT COUNT(*) AS orphaned_records \n",
    "                        FROM Products prod\n",
    "                        WHERE NOT EXISTS (SELECT 1\n",
    "                                            FROM Suppliers sup\n",
    "                                            WHERE sup.SupplierID=prod.SupplierID)\n",
    "\"\"\")\n",
    "    orphaned_fks = sql_cursor.fetchone()[0]\n",
    "    if orphaned_fks > 0:\n",
    "        quality_issues.append(f\"    > {orphaned_fks:,} products with orphaned foreign keys...\")\n",
    "    # print(quality_issues) \n",
    "\n",
    "\n",
    "    print(\"\\nCHECK 7: FUTURES DATES CHECK\")\n",
    "    sql_cursor.execute(\"\"\" SELECT COUNT(*) as future_dates_count \n",
    "                            FROM Customers\n",
    "                            WHERE CreatedDate > GETDATE()\n",
    "\"\"\")\n",
    "    future_dates = sql_cursor.fetchone()[0]\n",
    "    if future_dates > 0:\n",
    "        quality_issues.append(f\"    > {future_dates:,} customers with future creations data later than current date...\")\n",
    "    # print(quality_issues)\n",
    "\n",
    "    if quality_issues:\n",
    "        print(\"\\nData quality issues found (will migrate as-is)\")\n",
    "        for issue in quality_issues:\n",
    "            print(issue)\n",
    "    else:\n",
    "        print(\"No data qualituy issues identified!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] ===> Unexpected issue: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0f295f",
   "metadata": {},
   "source": [
    "## 6. Get table schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ba97fa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "ANALYZE TABLE SCHEMA\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 65)\n",
    "print(\"ANALYZE TABLE SCHEMA\")\n",
    "print(\"=\" * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "20988c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Categories\n",
      "----------\n",
      "    COLUMN_NAME DATA_TYPE  CHARACTER_MAXIMUM_LENGTH IS_NULLABLE\n",
      "0    CategoryID       int                       NaN          NO\n",
      "1  CategoryName  nvarchar                      50.0         YES\n",
      "2   Description  nvarchar                      -1.0         YES\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Suppliers\n",
      "----------\n",
      "    COLUMN_NAME DATA_TYPE  CHARACTER_MAXIMUM_LENGTH IS_NULLABLE\n",
      "0    SupplierID       int                       NaN          NO\n",
      "1  SupplierName  nvarchar                     150.0         YES\n",
      "2   ContactName  nvarchar                     100.0         YES\n",
      "3       Country  nvarchar                     100.0         YES\n",
      "4         Phone  nvarchar                      20.0         YES\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Customers\n",
      "----------\n",
      "    COLUMN_NAME DATA_TYPE  CHARACTER_MAXIMUM_LENGTH IS_NULLABLE\n",
      "0    CustomerID       int                       NaN          NO\n",
      "1  CustomerName  nvarchar                     100.0         YES\n",
      "2         Email  nvarchar                     100.0         YES\n",
      "3         Phone  nvarchar                      20.0         YES\n",
      "4       Country  nvarchar                     100.0         YES\n",
      "5   CreatedDate  datetime                       NaN         YES\n",
      "6      IsActive       bit                       NaN         YES\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Products\n",
      "----------\n",
      "     COLUMN_NAME DATA_TYPE  CHARACTER_MAXIMUM_LENGTH IS_NULLABLE\n",
      "0      ProductID       int                       NaN          NO\n",
      "1    ProductName  nvarchar                     200.0         YES\n",
      "2     CategoryID       int                       NaN         YES\n",
      "3     SupplierID       int                       NaN         YES\n",
      "4      UnitPrice     money                       NaN         YES\n",
      "5  StockQuantity       int                       NaN         YES\n",
      "6    CreatedDate  datetime                       NaN         YES\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_21680\\3909585054.py:20: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(schema_query, sql_conn)\n",
      "C:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_21680\\3909585054.py:20: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(schema_query, sql_conn)\n",
      "C:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_21680\\3909585054.py:20: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(schema_query, sql_conn)\n",
      "C:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_21680\\3909585054.py:20: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(schema_query, sql_conn)\n"
     ]
    }
   ],
   "source": [
    "table_schema = {}\n",
    "\n",
    "\n",
    "try:\n",
    "    for table in tables_to_migrate:\n",
    "        schema_query = f\"\"\"\n",
    "            SELECT \n",
    "                COLUMN_NAME, \n",
    "                DATA_TYPE, \n",
    "                CHARACTER_MAXIMUM_LENGTH,\n",
    "                IS_NULLABLE\n",
    "            FROM \n",
    "                INFORMATION_SCHEMA.COLUMNS\n",
    "            WHERE \n",
    "                table_name = '{table}'\n",
    "            ORDER BY \n",
    "                ORDINAL_POSITION\n",
    "\n",
    "\"\"\"\n",
    "        schema_df = pd.read_sql(schema_query, sql_conn)\n",
    "        print(f\"\\n{table}\")\n",
    "        print(\"-\" * 10)\n",
    "        print(schema_df)\n",
    "        table_schema[table] = schema_df\n",
    "        print(\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648ed465",
   "metadata": {},
   "source": [
    "# 7. Define data type mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c8a0736c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "DATA TYPE MAPPING\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 65)\n",
    "print(\"DATA TYPE MAPPING\")\n",
    "print(\"=\" * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "351101b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_mapping = {\n",
    "    'int': 'INTEGER',\n",
    "    'bigint': 'BIGINT',\n",
    "    'smallint': 'SMALLINT',\n",
    "    'tinyint': 'SMALLINT',\n",
    "    'bit': 'BOOLEAN',\n",
    "    'decimal': 'NUMERIC',\n",
    "    'numeric': 'NUMERIC',\n",
    "    'money': 'NUMERIC(19,4)',\n",
    "    'smallmoney': 'NUMERIC(10,4)',\n",
    "    'float': 'DOUBLE PRECISION',\n",
    "    'real': 'REAL',\n",
    "    'datetime': 'TIMESTAMP',\n",
    "    'datetime2': 'TIMESTAMP',\n",
    "    'smalldatetime': 'TIMESTAMP',\n",
    "    'date': 'DATE',\n",
    "    'time': 'TIME',\n",
    "    'char': 'CHAR',\n",
    "    'varchar': 'VARCHAR',\n",
    "    'nchar': 'CHAR',\n",
    "    'nvarchar': 'VARCHAR',\n",
    "    'text': 'TEXT',\n",
    "    'ntext': 'TEXT'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9bd64f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Server to PostgreSQL type mapping \n",
      "\n",
      "    int               --->      INTEGER\n",
      "    bigint            --->      BIGINT\n",
      "    smallint          --->      SMALLINT\n",
      "    tinyint           --->      SMALLINT\n",
      "    bit               --->      BOOLEAN\n",
      "    decimal           --->      NUMERIC\n",
      "    numeric           --->      NUMERIC\n",
      "    money             --->      NUMERIC(19,4)\n",
      "    smallmoney        --->      NUMERIC(10,4)\n",
      "    float             --->      DOUBLE PRECISION\n",
      "    real              --->      REAL\n",
      "    datetime          --->      TIMESTAMP\n",
      "    datetime2         --->      TIMESTAMP\n",
      "    smalldatetime     --->      TIMESTAMP\n",
      "    date              --->      DATE\n",
      "    time              --->      TIME\n",
      "    char              --->      CHAR\n",
      "    varchar           --->      VARCHAR\n",
      "    nchar             --->      CHAR\n",
      "    nvarchar          --->      VARCHAR\n",
      "    text              --->      TEXT\n",
      "    ntext             --->      TEXT\n"
     ]
    }
   ],
   "source": [
    "print(\"SQL Server to PostgreSQL type mapping \")\n",
    "print()\n",
    "\n",
    "for sql_type, pg_type in list(type_mapping.items()):\n",
    "    print(f\"    {sql_type:17} --->      {pg_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cc24b4",
   "metadata": {},
   "source": [
    "## 8. Create tables in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "aa660189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "CREATE TABLES IN POSTGRES\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 65)\n",
    "print(\"CREATE TABLES IN POSTGRES\")\n",
    "print(\"=\" * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "38fbf4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " + =======================================================\n",
      "[SUCCESS] ---> All tables created successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for table in tables_to_migrate:\n",
    "\n",
    "        schema = table_schema[table]\n",
    "\n",
    "        pg_table = table.lower()\n",
    "\n",
    "        pg_cursor.execute(f\"DROP TABLE IF EXISTS {pg_table} CASCADE\")\n",
    "\n",
    "        column_definitions = []\n",
    "\n",
    "        for idx, row in schema.iterrows():\n",
    "            col_name = row['COLUMN_NAME'].lower()\n",
    "            sql_type = row['DATA_TYPE']\n",
    "\n",
    "            base_type = sql_type.lower()\n",
    "            pg_type = type_mapping.get(base_type, 'TEXT')      \n",
    "\n",
    "            condition_1 = idx == 0                      # Must be first column in the table\n",
    "            condition_2 = col_name.endswith('id')       # Must end with ID\n",
    "            condition_3 = 'int' in sql_type.lower()     # Must be INT data type\n",
    "\n",
    "            if condition_1 and condition_2 and condition_3:\n",
    "                column_definitions.append(f\"{col_name} SERIAL PRIMARY KEY\")\n",
    "            else:\n",
    "                column_definitions.append(f\"{col_name} {pg_type}\")\n",
    "\n",
    "    \n",
    "        column_string = \",\\n        \".join(column_definitions)\n",
    "        create_query =  f\"\"\" \n",
    "        CREATE TABLE {pg_table} (\n",
    "            {column_string}\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        pg_cursor.execute(create_query)\n",
    "        pg_conn.commit()\n",
    "    \n",
    "    print(\"\\n + \" + \"=\" * 55)\n",
    "    print(\"[SUCCESS] ---> All tables created successfully!\")\n",
    "\n",
    "\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"Postgres experienced an error while creating a table: {e}\")\n",
    "    pg_conn.rollback()\n",
    "    raise \n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected issue: {e}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe217296",
   "metadata": {},
   "source": [
    "# 9. Test migration with one table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f4b807a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "TESTING MIGRATION (SINGLE TABLE)\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 65)\n",
    "print(\"TESTING MIGRATION (SINGLE TABLE)\")\n",
    "print(\"=\" * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6048dc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table = 'Customers'\n",
    "pg_table = test_table.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d5867fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Read from SQL Server... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_21680\\1989631993.py:4: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  test_df = pd.read_sql(extract_query, sql_conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Read 900000 rows\n",
      "2. Transforming data types...\n",
      "[SUCCESS] ---> Converted IsActive: BIT ---> BOOLEAN\n",
      "3. Prepare the data for loading\n",
      "        Prepared 900,000 rows\n",
      "4. Insert data into PostgreSQL...\n",
      "Loaded 900,000 rows\n",
      "5. Verifying...\n",
      "[SUCCESS] --> Verification passed: 900,000 == 900,000 \n",
      "\n",
      " Customers migration test successfully completed!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"1. Read from SQL Server... \")\n",
    "    extract_query = f\"SELECT * FROM {pg_table}\"\n",
    "    test_df = pd.read_sql(extract_query, sql_conn)\n",
    "\n",
    "    print(f\"        Read {len(test_df)} rows\")\n",
    "\n",
    "\n",
    "    print(\"2. Transforming data types...\")\n",
    "\n",
    "    if 'IsActive' in test_df.columns:\n",
    "        test_df['IsActive'] = test_df['IsActive'].astype('bool')\n",
    "        print(\"[SUCCESS] ---> Converted IsActive: BIT ---> BOOLEAN\")\n",
    "\n",
    "\n",
    "    print(\"3. Prepare the data for loading\")\n",
    "    data_tuples = [tuple(row) for row in test_df.to_numpy()]\n",
    "\n",
    "    columns = [col.lower() for col in test_df.columns]\n",
    "\n",
    "    columns_string = ', '.join(columns)\n",
    "\n",
    "    placeholders = ', '.join(['%s'] * len(columns))\n",
    "\n",
    "    insert_query = f\"\"\"\n",
    "        INSERT INTO {pg_table} ({columns_string})\n",
    "        VALUES %s\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"        Prepared {len(data_tuples):,} rows\")\n",
    "\n",
    "\n",
    "    print(\"4. Insert data into PostgreSQL...\")\n",
    "    execute_values(pg_cursor, insert_query, data_tuples, page_size=1000)\n",
    "    pg_conn.commit()\n",
    "\n",
    "    print(f\"Loaded {len(data_tuples):,} rows\")\n",
    "\n",
    "\n",
    "    print(\"5. Verifying...\")\n",
    "    pg_cursor.execute(f\"SELECT COUNT(*) AS total_rows FROM {pg_table}\")\n",
    "    pg_count = pg_cursor.fetchone()[0]\n",
    "\n",
    "    sql_count = baseline_counts[test_table]\n",
    "\n",
    "    if pg_count == sql_count:\n",
    "        print(f\"[SUCCESS] --> Verification passed: {pg_count:,} == {sql_count:,} \")\n",
    "    else:\n",
    "        print(f\"[FAILED] --> Count mismatch: {pg_count:,} != {sql_count:,}\")\n",
    "\n",
    "    \n",
    "    print(f\"\\n {test_table} migration test successfully completed!\")\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    pg_conn.rollback()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70f0e7c",
   "metadata": {},
   "source": [
    "## 10. Migrate remaining tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "dbb3e170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "MIGRATE REMAINING TABLES\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 65)\n",
    "print(\"MIGRATE REMAINING TABLES\")\n",
    "print(\"=\" * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ed2e6887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Migrating Categories --> categories...\n",
      "1. Reading from SQL Server...\n",
      "     Read 8 rows\n",
      "\n",
      "\n",
      "2. Preparing data...\n",
      "        Prepared 8 rows\n",
      "\n",
      "\n",
      "3. Processing bulk load...\n",
      "[SUCCESS] ---> Loaded 8 rows\n",
      "\n",
      "\n",
      "5. Verifying...\n",
      "[SUCCESS] --> Verification passed: 8 == 8 \n",
      "\n",
      " Categories migration successfully completed!\n",
      "Migrating Suppliers --> suppliers...\n",
      "1. Reading from SQL Server...\n",
      "     Read 5,000 rows\n",
      "\n",
      "\n",
      "2. Preparing data...\n",
      "        Prepared 5,000 rows\n",
      "\n",
      "\n",
      "3. Processing bulk load...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_21680\\3996697596.py:11: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  sql_df = pd.read_sql(extract_query, sql_conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] ---> Loaded 5,000 rows\n",
      "\n",
      "\n",
      "5. Verifying...\n",
      "[SUCCESS] --> Verification passed: 5,000 == 5,000 \n",
      "\n",
      " Suppliers migration successfully completed!\n",
      "Migrating Products --> products...\n",
      "1. Reading from SQL Server...\n",
      "     Read 150,000 rows\n",
      "\n",
      "\n",
      "2. Preparing data...\n",
      "        Prepared 150,000 rows\n",
      "\n",
      "\n",
      "3. Processing bulk load...\n",
      "[SUCCESS] ---> Loaded 150,000 rows\n",
      "\n",
      "\n",
      "5. Verifying...\n",
      "[SUCCESS] --> Verification passed: 150,000 == 150,000 \n",
      "\n",
      " Products migration successfully completed!\n"
     ]
    }
   ],
   "source": [
    "remaining_tables = [t for t in tables_to_migrate if t != 'Customers']\n",
    "\n",
    "for table in remaining_tables:\n",
    "    pg_table = table.lower()\n",
    "\n",
    "    print(f\"Migrating {table} --> {pg_table}...\")\n",
    "\n",
    "    try:\n",
    "        print(\"1. Reading from SQL Server...\")\n",
    "        extract_query = f\"SELECT * FROM {table}\"\n",
    "        sql_df = pd.read_sql(extract_query, sql_conn)\n",
    "        print(f\"     Read {len(sql_df):,} rows\\n\\n\")\n",
    "\n",
    "        print(\"2. Preparing data...\")\n",
    "        data_tuples = [tuple(row) for row in sql_df.to_numpy()]\n",
    "        columns = [col.lower() for col in sql_df.columns]\n",
    "        columns_string = ', '.join(columns)\n",
    "        insert_query = f\"\"\"\n",
    "                INSERT INTO {pg_table} ({columns_string})\n",
    "                VALUES %s\n",
    "\"\"\"\n",
    "        print(f\"        Prepared {len(data_tuples):,} rows\\n\\n\")\n",
    "\n",
    "\n",
    "        print(\"3. Processing bulk load...\")\n",
    "        execute_values(pg_cursor, insert_query, data_tuples, page_size=1000)\n",
    "        pg_conn.commit()\n",
    "\n",
    "        print(f\"[SUCCESS] ---> Loaded {len(data_tuples):,} rows\\n\\n\")\n",
    "\n",
    "\n",
    "        print(\"5. Verifying...\")\n",
    "        pg_cursor.execute(f\"SELECT COUNT(*) AS total_rows FROM {pg_table}\")\n",
    "        pg_count = pg_cursor.fetchone()[0]\n",
    "\n",
    "        sql_count = baseline_counts[table]\n",
    "\n",
    "        if pg_count == sql_count:\n",
    "            print(f\"[SUCCESS] --> Verification passed: {pg_count:,} == {sql_count:,} \")\n",
    "        else:\n",
    "            print(f\"[FAILED] --> Count mismatch: {pg_count:,} != {sql_count:,}\")\n",
    "\n",
    "        \n",
    "        print(f\"\\n {table} migration successfully completed!\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to migrate '{table}: {e}' \")\n",
    "        pg_conn.rollback()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568029fc",
   "metadata": {},
   "source": [
    "# 11. Run post-migration validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1c4efb",
   "metadata": {},
   "source": [
    "### -- 1. Row counts checks\n",
    "\n",
    "##### SUCCESS: MATCHES SQL SERVER\n",
    "\n",
    "\n",
    "### -- 2. Data type checks\n",
    "\n",
    "\n",
    "##### SUCCESS: MATCHES SQL SERVER\n",
    "\n",
    "\n",
    "### -- 3. Primary key checks (duplicates)\n",
    "\n",
    "\n",
    "##### SUCCESS: MATCHES SQL SERVER\n",
    "\n",
    "\n",
    "### -- 4. Referential integrity checks (duplicates)\n",
    "\n",
    "\n",
    "##### SUCCESS: MATCHES SQL SERVER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba3a799",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c971d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00241ece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sqlserver-to-postgres",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
